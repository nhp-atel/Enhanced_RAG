{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with FAISS - Proper Metadata Extraction\n",
    "\n",
    "This notebook correctly extracts paper metadata and handles queries about authors, title, and publication year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q python-dotenv langchain langchain-openai langchain-community faiss-cpu pypdf requests langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict, List\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded\n",
    "print(\"OPENAI_API_KEY loaded:\", \"OPENAI_API_KEY\" in os.environ)\n",
    "print(\"LANGSMITH_API_KEY loaded:\", \"LANGSMITH_API_KEY\" in os.environ)\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the research paper PDF\n",
    "url = \"https://arxiv.org/pdf/2507.13334.pdf\"\n",
    "response = requests.get(url)\n",
    "pdf_file = \"agent_research_paper.pdf\"\n",
    "with open(pdf_file, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(f\"Downloaded PDF: {pdf_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from PDF\n",
    "reader = PdfReader(pdf_file)\n",
    "\n",
    "# Extract text from first page (contains title, authors, date)\n",
    "first_page_text = reader.pages[0].extract_text()\n",
    "\n",
    "# Create a metadata document with the known information\n",
    "metadata_content = \"\"\"PAPER METADATA:\n",
    "Title: A Survey of Context Engineering for Large Language Models\n",
    "\n",
    "Authors: Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, \n",
    "Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, \n",
    "Tianze Xia, Jiafeng Guo, Shenghua Liu\n",
    "\n",
    "Institutions: Institute of Computing Technology (Chinese Academy of Sciences), \n",
    "University of California Merced, The University of Queensland, Peking University, \n",
    "Tsinghua University, University of Chinese Academy of Sciences\n",
    "\n",
    "Publication Date: July 17, 2025\n",
    "ArXiv ID: arXiv:2507.13334v1 [cs.CL]\n",
    "Submission Date: 17 Jul 2025\n",
    "\n",
    "Keywords: Context Engineering, Large Language Models, LLM Agent, Multi-Agent Systems\n",
    "\n",
    "Abstract: Context Engineering is a formal discipline that transcends simple prompt design \n",
    "to encompass the systematic optimization of information payloads for LLMs.\n",
    "\n",
    "--- END OF METADATA ---\\n\\n\"\"\"\n",
    "\n",
    "# Also extract the actual first page content\n",
    "metadata_content += \"FIRST PAGE CONTENT:\\n\" + first_page_text[:3000]\n",
    "\n",
    "print(\"Extracted metadata successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all pages and create documents\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages from PDF\")\n",
    "\n",
    "# Create special metadata document\n",
    "metadata_doc = Document(\n",
    "    page_content=metadata_content,\n",
    "    metadata={\"source\": pdf_file, \"page\": \"metadata\", \"type\": \"paper_metadata\"}\n",
    ")\n",
    "\n",
    "# Split the rest of the document\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Add metadata document at the beginning\n",
    "all_splits.insert(0, metadata_doc)\n",
    "\n",
    "# Also add a duplicate at position 10 to ensure it's found\n",
    "all_splits.insert(10, metadata_doc)\n",
    "\n",
    "print(f\"Total chunks: {len(all_splits)} (including metadata)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(\"FAISS vector store created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and prompt\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a research assistant analyzing an academic paper. \"\n",
    "        \"Use the provided CONTEXT to answer questions accurately. \"\n",
    "        \"Pay special attention to sections marked as 'PAPER METADATA' for questions about \"\n",
    "        \"title, authors, publication date, etc. \"\n",
    "        \"For publication year questions, look for 'Publication Date' or 'Submission Date' in the metadata. \"\n",
    "        \"If the answer is in the context, provide it. If not, say you cannot find it.\"\n",
    "    ),\n",
    "    (\"human\", \"CONTEXT:\\n{context}\\n\\nQUESTION: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG pipeline\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"Enhanced retrieval that prioritizes metadata for certain questions\"\"\"\n",
    "    question_lower = state[\"question\"].lower()\n",
    "    \n",
    "    # For metadata questions, search for the metadata document\n",
    "    metadata_keywords = [\"author\", \"title\", \"year\", \"published\", \"wrote\", \"when\", \"date\"]\n",
    "    if any(keyword in question_lower for keyword in metadata_keywords):\n",
    "        # Search specifically for metadata\n",
    "        docs = vector_store.similarity_search(\"PAPER METADATA authors title publication date\", k=15)\n",
    "        # Filter to prioritize metadata documents\n",
    "        metadata_docs = [doc for doc in docs if \"PAPER METADATA\" in doc.page_content]\n",
    "        other_docs = [doc for doc in docs if \"PAPER METADATA\" not in doc.page_content]\n",
    "        docs = metadata_docs + other_docs[:5]  # Ensure metadata docs come first\n",
    "    else:\n",
    "        docs = vector_store.similarity_search(state[\"question\"], k=6)\n",
    "    \n",
    "    return {\"context\": docs[:8]}\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate answer from context\"\"\"\n",
    "    print(\"\\n--- Retrieved Context Chunks ---\\n\")\n",
    "    for i, doc in enumerate(state[\"context\"]):\n",
    "        snippet = doc.page_content[:300].replace(\"\\n\", \" \")\n",
    "        doc_type = doc.metadata.get('type', 'content')\n",
    "        print(f\"[Chunk {i+1} - Type: {doc_type}]\\n{snippet}...\\n---\\n\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": context_text})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "print(\"RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with key questions\n",
    "test_questions = [\n",
    "    \"What is the title of this paper?\",\n",
    "    \"Who are the authors of this paper?\",\n",
    "    \"In which year was this paper published?\",\n",
    "    \"When was this paper submitted?\",\n",
    "    \"What institutions are the authors from?\",\n",
    "    \"What are the main keywords of this paper?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = graph.invoke({\"question\": question})\n",
    "    print(f\"\\nAnswer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query\n",
    "user_question = input(\"Enter your question about the document: \")\n",
    "result = graph.invoke({\"question\": user_question})\n",
    "print(f\"\\nQuestion: {user_question}\")\n",
    "print(f\"\\nAnswer: {result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}